---
title: "P8105_hw5"
author: "Hongzhu Ren"
date: "2023-11-15"
output: github_document
---
```{r setup}
library(tidyverse)
library(readr)
```

# Problem 1
```{r homicide data}
## read data
data_url <- "https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv"
homicide <- read_csv(data_url, na = c("", "NA", "Unknown"))
```

The raw data contains `r nrow(homicide)` observations of `r ncol(homicide)` variables, containing `r colnames(homicide)`. Variables with prefix "victim" described the characteristics of victim, `disposition` describes the outcome of the case. 

```{r homicide tidy}
## add city_state variable
homicide_tidy <- homicide|>
  mutate(
    city_state = str_c(city,state,sep = ",")
  )|>
  filter(city_state != "Tulsa,AL") 

```

```{r homicide unsolved}
## get homicide unsolved in each city
homicide_unsolved <- homicide_tidy |>
  group_by(city_state)|>
  summarise(
    homicide = n(),
    unsolved = sum(disposition %in% c("Closed without arrest", "Open/No arrest"))
  )|>
  arrange(unsolved)
```

The city with lowest unsolved cases is `r homicide_unsolved|>pull(city_state)|>first()`,the city with highest unsolved cases is `r homicide_unsolved|>pull(city_state)|>last()`

```{r Baltimore,MD}
Bal_test <- prop.test(
  homicide_unsolved |>filter(city_state == "Baltimore,MD")|>pull(unsolved),
   homicide_unsolved |>filter(city_state == "Baltimore,MD")|>pull(homicide))

Bal_test |> broom::tidy()|>
  select(estimate,conf.low,conf.high)|>
  knitr::kable()
```

The outcome of prop.test is shown in the table above.

```{r city test}
city_test <- homicide_unsolved |>
  mutate(
    prop_test = map2(unsolved,homicide, \(x,y) prop.test(x=x,n=y)),
    tidy_test = map(prop_test, broom::tidy)
  )|>
  unnest(tidy_test)|>
  select(city_state,estimate,conf.low,conf.high)|>
  mutate(city_state = fct_reorder(city_state,estimate))
```

```{r estimate plot}
city_test |>
  ggplot(aes(y=estimate,x=city_state))+
  geom_point()+
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

The plot suggests a widely ranged rate of unsolved homicides. Chicago has a high rate with low CI, indicating this place is at high risk of homicide.

# Problem 2
We first generate files names and form a initial data frame containg the information of group and id.
```{r}
## generate file names
file_names <- list.files(path = "./data")
file_path <- str_c("./data",file_names,sep = "/")

## generate group and id
group_id <- 
  tibble(
    group_id = str_remove(file_names,".csv")
  )
```

Read longitudinal data into the initial data frame with map function and use pivot_longer to get longitudinal dataset for plotting.

```{r}
## read longitudinal observations data
longitudinal <- mutate(separate(group_id, col = group_id,into = c("group","id"),sep = "_"))|>
  mutate(
    observation = map(file_path,read_csv)
    )|>
  unnest(observation)|> 
  pivot_longer(
    cols = starts_with("week"),
    names_prefix = "week_",
    names_to = "week",
    values_to = "observation" 
  )

## make spaghetti plot
longitudinal |>
  ggplot(aes(x = week, y = observation, group = id, color = group))+
  geom_line()+
  facet_wrap(~group)
```

From the spaghetti plot, we can see that experiment group has higher average value of observations and tend to have a higher rate of increasing over time.

# Problem 3
We first write a function to generate estimation and p.value for each mu.
```{r result function}
## set.seed
set.seed(0)

## function to get estimate and p_value of 5000 samples
one_sample_t <- function(mu){
  n = 30
  sigma = 5

  data = list(
    testdata = list(),
    test_result = list()
  )

  for (i in 1:5000) {
    data$testdata[[i]] <- rnorm(n,mean = mu,sd = sigma)
    data$test_result[[i]] <- t.test(data$testdata[[i]])|>
    broom::tidy()
  }

  estimate_result <- tibble(
    data$test_result
  )|>
    unnest()|>
    mutate(
      mu = mu
    )|>
    select(mu,estimate,p.value)
  
  return(estimate_result)
}
```

`mu0_result` contains the estimates and p.values of t.test when mu=0

```{r mu_0}
mu0_result <- one_sample_t(0)
```

`mu16_result` contains the estimates and p.values of t.test when mu=1,2,3,4,5,6.

```{r mu1_6}
mu1_6 <- c(1,2,3,4,5,6)
mu16_result <- map(mu1_6,\(mu) one_sample_t(mu))|>
  tibble()|>
  unnest()
```

combines two results and get the `final_result`

```{r final_result}
final_result <- bind_rows(mu0_result,mu16_result)
```

Befor making plot, we need to add reject tag

```{r plot_tidy}
final_plot <- final_result |>
  mutate(
    reject = case_when(
      p.value<0.05 ~ 1,
      p.value>=0.05 ~ 0
    )
  )
```

```{r plot_effect_size}
final_plot |>
  group_by(mu)|>
  summarise(
    reject_portion = sum(reject==1)/n()
  )|>
  ggplot(aes(x = mu,y = reject_portion))+
  geom_point()+
  geom_line()+
  labs(
    title = "Reject portion of each mu"
  )
```

Since sigma is given and fixed, the effect size solely depends on the difference of mu and H0 hypothesis. With bigger mu comes a bigger effect size, and larger effect size contributes to a higher power.

Now we make the plot of estimated mu and true value.

```{r plot_mean_estimate_1}
final_plot |>
  group_by(mu)|>
  summarise(
    mu_hat = mean(estimate)
  )|>
  ggplot(aes(x = mu,y = mu_hat))+
  geom_point()+
  geom_line()+
  geom_abline(slope = 1,intercept = 0,color="red")+
  labs(
    title = "mu_hat and mu"
  )
```

The red line is the reference for the identical relation of mu_hat and mu. The estimated mus from completed dataset are almost identical as the true value.

```{r plot_mean_estimate_2}
final_plot |>
  filter(
    reject==1
  )|>
  group_by(mu)|>
  summarise(
    mu_hat = mean(estimate)
  )|>
  ggplot(aes(x = mu,y = mu_hat))+
  geom_point()+
  geom_line()+
  geom_abline(slope = 1,intercept = 0,color="red")+
  labs(
    title = "mu_hat(H0 rejected) and mu"
  )
```

The red line is the reference for the identical relation of mu_hat and mu.

For estimated mu from rejected, there is a significant difference when mu has true value of 1,2,3. 

* For small mu. The differences are due to the low test power resulted from low effect size. In datasets of small mu, only those with larger sample means can be rejected. This will result in biased estimatation, thus the mean of rejected data tends to be larger than the true value.

* For mu=0, the distribution is balanced even in rejected group, thus the group mean almost equals to the true value.

* lFor large mu, the effect size and test power will be large as well. The null hypothesis will be rejected at a probability that almost equals 1. Thus the whole dataset will be accounted into the estimation and eventually get unbiased estimation.